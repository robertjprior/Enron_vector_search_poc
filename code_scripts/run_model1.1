#%%
import re
import nltk
## for plotting
import matplotlib.pyplot as plt
#import seaborn as sns
## for d2v
import gensim
import gensim.downloader as gensim_api
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import strip_tags
from gensim.models.doc2vec import TaggedDocument, Doc2Vec

import pandas as pd
import numpy as np
from sklearn import metrics, manifold


#CODE PROCESSES ABOUT 233 OBSERVATIONS/SECOND with a 517k dataset
#%%
#load necessary files:
nlp = gensim_api.load("glove-wiki-gigaword-300")
df = pd.read_csv("../data/emails_clean.csv")

#%%
#Useful functions
text_col = 'body'
self_defined_categories = ['fraud']

# %%
## Functions
def get_similar_words(lst_words, top, nlp):
    lst_out = lst_words
    for tupla in nlp.most_similar(lst_words, topn=top):
        lst_out.append(tupla[0])
    return list(set(lst_out))

def tokenize(doc):
    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)
## Create Dictionary {category:[keywords]}

#%%
#Data Processing (Tokenization)
dic_clusters = {}
#below is where if you wanted multiple labels you can add repeat the line below with the dictionary key being your category and then change how we save the "y" variable and then measure cosine similarity
dic_clusters["fraud"] = get_similar_words(['fraud','cover-up','illegal','hide'], top=10, nlp=nlp)

#tokenize our text data
df['tagged_docs'] = df.apply(lambda row: TaggedDocument(tokenize(str(row[text_col])), [str(row.name)]), axis=1)

#%%
#Modeling
model = Doc2Vec(df['tagged_docs'], vector_size=150, window=2, min_count=2, workers=-1)
y = model.infer_vector(dic_clusters['fraud']) #creating a custom label word vector embedding

#%%
#Inference
X = df['tagged_docs'].apply(lambda row: model.infer_vector(doc_words=row[0]))
X = np.stack(X.values)

similarities = metrics.pairwise.cosine_similarity(X, y.reshape(1,-1)) # could do pearson coefficient instead https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder
#similar_docs = model.dv.most_similar(positive=[y], topn=20)
pd.DataFrame(similarities).describe()
df['similarities'] = similarities

#%%
#Plotting
#graph the ratings and find an intuitive cutoff value
plt.hist(similarities, 50, density=True, facecolor='g', alpha=0.75)
plt.show()
# %%
bad_filter = df['similarities'] > 0.20
testing = df.loc[bad_filter, :].groupby(['From']).agg({'similarities':['mean', 'count']})
testing.columns = ["_".join(x) for x in testing.columns.ravel()]
testing = testing.sort_values(["similarities_count", "similarities_mean"], ascending=False)
testing
#.columns
# %%
df.drop(labels= "tagged_docs", inplace=True, axis=1)
df.to_csv(r'../data/emails_clean.csv', index = False)
# %%
to_remove = 'confidentiality notice the information in this email may be confidential andor privileged this email is intended to be reviewed by only the individual or organization named above if you are not the intended recipient or an authorized representative of the intended recipient you are hereby notified that any review dissemination or copying of this email and its attachments if any or the information contained herein is prohibited if you have received this email in error please immediately notify the sender by return email and delete this email from your system thank you confidentiality notice the information in this email may be confidential andor privileged this email is intended to be reviewed by only the individual or organization named above if you are not the intended recipient or an authorized representative of the intended recipient you are hereby notified that any review dissemination or copying of this email and its attachments if any or the information contained herein is prohibited if you have received this email in error please immediately notify the sender by return email and delete this email from your system thank you'
